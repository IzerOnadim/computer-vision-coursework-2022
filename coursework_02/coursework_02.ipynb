{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coursework 2: Fish Classification\n",
    "\n",
    "Created by Athanasios Vlontzos and Wenjia Bai\n",
    "\n",
    "In this coursework, you will be exploring the application of convolutional neural networks for image classification tasks. As opposed to standard applications such as object or face classification, we will be dealing with a slightly different domain, fish classification for precision fishing.\n",
    "\n",
    "In precision fishing, engineers and fishmen collaborate to extract a wide variety of information about the fish, their species and wellbeing etc. using data from satellite images to drones surveying the fisheries. The goal of precision fishing is to provide the marine industry with information to support their decision making processes.\n",
    "\n",
    "Here your will develop an image classification model that can classify fish species given input images. It consists of two tasks. The first task is to train a model for the following species:\n",
    "- Black Sea Sprat\n",
    "- Gilt-Head Bream\n",
    "- Shrimp\n",
    "- Striped Red Mullet\n",
    "- Trout\n",
    "\n",
    "The second task is to finetune the last layer of the trained model to adapt to some new species, including:\n",
    "- Hourse Mackerel\n",
    "- Red Mullet\n",
    "- Red Sea Bream\n",
    "- Sea Bass\n",
    "\n",
    "You will be working using a large-scale fish dataset [1].\n",
    "\n",
    "[1] O. Ulucan, D. Karakaya and M. Turkan. A large-scale dataset for fish segmentation and classification. Innovations in Intelligent Systems and Applications Conference (ASYU). 2020."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Download data.\n",
    "\n",
    "[Download the Data from here -- make sure you access it with your Imperial account.](https://imperiallondon-my.sharepoint.com/:f:/g/personal/av2514_ic_ac_uk/EkA9HyXVvgdFoLI4P_IfO1cBO_CsvY1KN4NE8iuD-s_VlA?e=Ip03rF)\n",
    "\n",
    "It is a ~2.5GB file. You can save the images and annotations directories in the same directory as this notebook or somewhere else.\n",
    "\n",
    "The fish dataset contains 9 species of fishes. There are 1,000 images for each fish species, named as %05d.png in each subdirectory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load the data. (15 Points)\n",
    "\n",
    "- Complete the dataset class with the skeleton below.\n",
    "- Add any transforms you feel are necessary.\n",
    "\n",
    "Your class should have at least 3 elements\n",
    "- An ```__init__``` function that sets up your class and all the necessary parameters.\n",
    "- An ```__len__``` function that returns the size of your dataset.\n",
    "- An ```__getitem__``` function that given an index within the limits of the size of the dataset returns the associated image and label in tensor form.\n",
    "\n",
    "You may add more helper functions if you want.\n",
    "\n",
    "In this section we are following the Pytorch [dataset](https://pytorch.org/vision/stable/datasets.html) class structure. You can take inspiration from their documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# We will start by building a dataset class using the following 5 species of fishes\n",
    "Multiclass_labels_correspondances = {\n",
    "    'Black Sea Sprat': 0,\n",
    "    'Gilt-Head Bream': 1,\n",
    "    'Shrimp': 2,\n",
    "    'Striped Red Mullet': 3,\n",
    "    'Trout': 4\n",
    "}\n",
    "\n",
    "inverted_labels_correspondances = dict(zip(\n",
    "    Multiclass_labels_correspondances.values(), Multiclass_labels_correspondances.keys()))\n",
    "\n",
    "# The 5 species will contain 5,000 images in total.\n",
    "# Let us split the 5,000 images into training (80%) and test (20%) sets\n",
    "def split_train_test(lendata, percentage=0.8):\n",
    "    #### ADD YOUR CODE HERE ####\n",
    "    return np.array_split(np.random.permutation(lendata), [round(percentage * lendata)])\n",
    "\n",
    "LENDATA = 5000\n",
    "np.random.seed(42)\n",
    "idxs_train, idxs_test = split_train_test(LENDATA, 0.8)\n",
    "\n",
    "# Implement the dataset class\n",
    "class FishDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 path_to_images,\n",
    "                 idxs_train,\n",
    "                 idxs_test,\n",
    "                 transform_extra=None,\n",
    "                 img_size=128,\n",
    "                 train=True):\n",
    "        # path_to_images: where you put the fish dataset\n",
    "        # idxs_train: training set indexes\n",
    "        # idxs_test: test set indexes\n",
    "        # transform_extra: extra data transform\n",
    "        # img_size: resize all images to a standard size\n",
    "        # train: return training set or test set\n",
    "        \n",
    "        # Load all the images and their labels\n",
    "        \n",
    "        # Resize all images to a standard size and \n",
    "        tfs = [transforms.Resize((img_size, img_size)), transforms.ToTensor()]\n",
    "        \n",
    "        if transform_extra is not None:\n",
    "            tfs.insert(0, transform_extra)\n",
    "        \n",
    "        transform = transforms.Compose(tfs)\n",
    "        \n",
    "        images = []\n",
    "        labels = []\n",
    "\n",
    "        for class_name, label in Multiclass_labels_correspondances.items():\n",
    "            files = glob.glob(f\"{path_to_images}/{class_name}/*.png\")\n",
    "            print(\"Class name: \" + class_name)\n",
    "            for file in files:\n",
    "                image = Image.open(file)\n",
    "                images.append(transform(image))\n",
    "                labels.append(label)\n",
    "                image.close()\n",
    "            \n",
    "        # Extract the images and labels with the specified file indexes      \n",
    "        indicies = idxs_train if train else idxs_test\n",
    "        self.images = [images[i] for i in indicies]\n",
    "        self.labels = [labels[i] for i in indicies]\n",
    "             \n",
    "    def __len__(self):\n",
    "        # Return the number of samples\n",
    "        #### ADD YOUR CODE HERE ####\n",
    "        return len(self.images)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        # Get an item using its index\n",
    "        # Return the image and its label\n",
    "        #### ADD YOUR CODE HERE ####\n",
    "        return self.images[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Explore the data. (15 Points)\n",
    "\n",
    "### Step 2.1: Data visualisation. (5 points)\n",
    "\n",
    "- Plot data distribution, i.e. the number of samples per class.\n",
    "- Plot 1 sample from each of the five classes in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Training set\n",
    "img_path = './fish-dataset'\n",
    "# dataset  = FishDataset(img_path, idxs_train, idxs_test, None, img_size=128, train=True)\n",
    "\n",
    "# # Plot the number of samples per class\n",
    "# sample_map = {}\n",
    "# first_of_each = []\n",
    "# for idx, label in enumerate(dataset.labels):\n",
    "#     if label in sample_map:\n",
    "#         sample_map[label] += 1 # Add to count\n",
    "#     else:\n",
    "#         first_of_each.append(idx) # New class\n",
    "#         sample_map[label] = 1\n",
    "\n",
    "# print(sorted(sample_map.items())) # Sanity check\n",
    "# classes = list([inverted_labels_correspondances[i] for i in sample_map.keys()])\n",
    "# count = list(sample_map.values())\n",
    "  \n",
    "# fig = plt.figure(figsize = (20, 10))\n",
    " \n",
    "# plt.bar(classes, count, color ='maroon', width = 0.4)\n",
    " \n",
    "# plt.xlabel(\"Class name\")\n",
    "# plt.ylabel(\"Number of classes\")\n",
    "# plt.title(\"Number of samples per class in training set\")\n",
    "# plt.show()\n",
    "        \n",
    "# # Plot 1 sample from each of the five classes in the training set\n",
    "# fig = plt.figure(figsize=(30, 20))\n",
    "# rows = 2\n",
    "# columns = 3\n",
    "# for i, idx in enumerate(first_of_each):\n",
    "#     fig.add_subplot(rows, columns, i + 1)\n",
    "#     plt.imshow(dataset.images[idx].permute(1, 2, 0).numpy())\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.2: Discussion. (10 points)\n",
    "\n",
    "* Is the dataset balanced?\n",
    "\n",
    "* Can you think of 3 ways to make the dataset balanced if it is not?\n",
    "\n",
    "* Is the dataset already pre-processed? If yes, how?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, we can see that the training dataset is roughly balanced. This is because the original dataset we start off with is perfectly balanced (it contains exactly 1000 of each type of fish), but when we randomly sample our training and test datasets, we may get a slightly higher or lower portion of the different types of fish in each set. However, since the psuedorandom generator used is good, the distribution of different classes in the training and test sets is almost balanced. For example, on one occasion the distribution looked like this: \n",
    "{0: 811, 1: 809, 2: 787, 3: 796, 4: 797}. \n",
    "\n",
    "If the dataset were not balanced, we could (1) upsample the minority class(es), (2) downsample the majority class(es), or (3) normalise our results (or possibly different combinations of these three).\n",
    "\n",
    "Yes, {mention rotation, all images same size (?), other transformations applied, etc.}."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Multiclass classification. (55 points)\n",
    "In this section we will try to make a multiclass classifier to determine the species of the fish.\n",
    "\n",
    "### Step 3.1: Define the model. (15 points)\n",
    "\n",
    "Design a neural network which consists of a number of convolutional layers and a few fully connected ones at the end.\n",
    "\n",
    "The exact architecture is up to you but you do NOT need to create something complicated. For example, you could design a LeNet insprired network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, output_dims = 1):\n",
    "        super(Net, self).__init__()\n",
    "        self.cnn_layers = nn.Sequential(\n",
    "            # Conv layer 1\n",
    "            nn.Conv2d(3, 30, kernel_size=(5, 5), stride=1, padding=1),\n",
    "            nn.BatchNorm2d(30),                         # Batch Normalisation \n",
    "            nn.ReLU(inplace=True),                      # ReLU\n",
    "            nn.MaxPool2d(kernel_size=(2, 2), stride=2), # MaxPool\n",
    "            # Conv layer 2\n",
    "            nn.Conv2d(30, 40, kernel_size=(5, 5), stride=1, padding=1),\n",
    "            nn.BatchNorm2d(40),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=(2, 2), stride=2),\n",
    "            # Conv layer 3\n",
    "            nn.Conv2d(40, 50, kernel_size=(5, 5), stride=1, padding=1),\n",
    "            nn.BatchNorm2d(50),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=(2, 2), stride=2),\n",
    "            # Conv layer 4\n",
    "            nn.Conv2d(50, 60, kernel_size=(5, 5), stride=1, padding=1),\n",
    "            nn.BatchNorm2d(60),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=(2, 2), stride=2),\n",
    "        )\n",
    "\n",
    "        self.linear_layers = nn.Sequential(\n",
    "            # Fully connected layer 1\n",
    "            nn.Linear(2160, 800),\n",
    "            nn.ReLU(),\n",
    "            # FCL 2\n",
    "            nn.Linear(800, 200),\n",
    "            nn.ReLU(),\n",
    "            # FCL 3\n",
    "            nn.Linear(200, 80),\n",
    "            nn.ReLU(),\n",
    "            # FCL 4 (output layer)\n",
    "            nn.Linear(80, len(Multiclass_labels_correspondances)),\n",
    "            # No need to apply Softmax - this is handled by nn.CrossEntropyLoss.\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Forward propagation\n",
    "        x = self.cnn_layers(x)    # Conv layers\n",
    "        x = x.view(x.size(0), -1) # Flatten\n",
    "        x = self.linear_layers(x) # Fully connected layers\n",
    "        return x\n",
    "\n",
    "# Since most of you use laptops, you may use CPU for training.\n",
    "# If you have a good GPU, you can set this to 'gpu'.\n",
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.2: Define the training parameters. (10 points)\n",
    "\n",
    "- Loss function\n",
    "- Optimizer\n",
    "- Learning Rate\n",
    "- Number of iterations\n",
    "- Batch Size\n",
    "- Other relevant hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class name: Black Sea Sprat\n",
      "Class name: Gilt-Head Bream\n",
      "Class name: Shrimp\n",
      "Class name: Striped Red Mullet\n",
      "Class name: Trout\n",
      "Class name: Black Sea Sprat\n",
      "Class name: Gilt-Head Bream\n",
      "Class name: Shrimp\n",
      "Class name: Striped Red Mullet\n",
      "Class name: Trout\n"
     ]
    }
   ],
   "source": [
    "# Network - send to GPU\n",
    "model = Net().to(device)\n",
    "\n",
    "# Loss function \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimiser and learning rate\n",
    "lr = 0.003\n",
    "wd = 0.001\n",
    "\n",
    "optimiser = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
    "\n",
    "# Number of iterations for training\n",
    "epochs = 20\n",
    "\n",
    "# Training batch size\n",
    "train_batch_size = 200\n",
    "\n",
    "# Based on the FishDataset, use the PyTorch DataLoader to load the data during model training\n",
    "train_dataset = FishDataset(img_path, idxs_train, idxs_test, None, img_size=128, train=True)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=train_batch_size)\n",
    "test_dataset = FishDataset(img_path, idxs_train, idxs_test, None, img_size=128, train=False)\n",
    "test_dataloader = DataLoader(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.3: Train the model. (15 points)\n",
    "\n",
    "Complete the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|█████▌                                                                                                        | 1/20 [00:04<01:29,  4.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Iteration 1: training loss = 1.3402687788009644 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|███████████                                                                                                   | 2/20 [00:09<01:24,  4.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Iteration 2: training loss = 0.4652406767010689 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 15%|████████████████▌                                                                                             | 3/20 [00:14<01:19,  4.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Iteration 3: training loss = 0.21317116543650627 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██████████████████████                                                                                        | 4/20 [00:18<01:15,  4.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Iteration 4: training loss = 0.16177434399724006 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 25%|███████████████████████████▌                                                                                  | 5/20 [00:23<01:10,  4.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Iteration 5: training loss = 0.0928288052789867 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|█████████████████████████████████                                                                             | 6/20 [00:28<01:05,  4.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Iteration 6: training loss = 0.04472007341682911 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 35%|██████████████████████████████████████▌                                                                       | 7/20 [00:32<01:01,  4.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Iteration 7: training loss = 0.0203716748743318 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████████████████████████████████████████████                                                                  | 8/20 [00:37<00:56,  4.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Iteration 8: training loss = 0.18114670431241392 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 45%|█████████████████████████████████████████████████▌                                                            | 9/20 [00:42<00:52,  4.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Iteration 9: training loss = 0.0639893785584718 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|██████████████████████████████████████████████████████▌                                                      | 10/20 [00:47<00:47,  4.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Iteration 10: training loss = 0.04225632396992296 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 55%|███████████████████████████████████████████████████████████▉                                                 | 11/20 [00:51<00:42,  4.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Iteration 11: training loss = 0.05742411683313549 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|█████████████████████████████████████████████████████████████████▍                                           | 12/20 [00:56<00:37,  4.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Iteration 12: training loss = 0.017409775755368173 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 65%|██████████████████████████████████████████████████████████████████████▊                                      | 13/20 [01:01<00:33,  4.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Iteration 13: training loss = 0.017969841172453017 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|████████████████████████████████████████████████████████████████████████████▎                                | 14/20 [01:06<00:28,  4.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Iteration 14: training loss = 0.01768118655309081 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 75%|█████████████████████████████████████████████████████████████████████████████████▊                           | 15/20 [01:10<00:23,  4.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Iteration 15: training loss = 0.09807769628241658 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|███████████████████████████████████████████████████████████████████████████████████████▏                     | 16/20 [01:15<00:18,  4.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Iteration 16: training loss = 0.09252880206331611 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 85%|████████████████████████████████████████████████████████████████████████████████████████████▋                | 17/20 [01:20<00:14,  4.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Iteration 17: training loss = 0.030562793486751616 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|██████████████████████████████████████████████████████████████████████████████████████████████████           | 18/20 [01:24<00:09,  4.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Iteration 18: training loss = 0.007510499964701012 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 95%|███████████████████████████████████████████████████████████████████████████████████████████████████████▌     | 19/20 [01:29<00:04,  4.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Iteration 19: training loss = 0.007667924708221108 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [01:34<00:00,  4.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Iteration 20: training loss = 0.003381311794510111 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    model.train()\n",
    "    loss_curve = []\n",
    "\n",
    "    for imgs, labs in train_dataloader:\n",
    "        # Get a batch of training data and train the model\n",
    "        imgs, labs = imgs.to(device), labs.to(device)\n",
    "        optimiser.zero_grad()\n",
    "        prediction = model(imgs)\n",
    "        loss = criterion(prediction, labs)\n",
    "        loss.retain_grad()\n",
    "        loss.backward()\n",
    "        optimiser.step()        \n",
    "        loss_curve.append(loss.item())\n",
    "    print(f'--- Iteration {epoch + 1}: training loss = {np.array(loss_curve).mean()} ---')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.4: Deploy the trained model onto the test set. (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  1.6066,  -6.4998,  10.7981,   4.5167, -14.7421]], device='cuda:0')\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# Deploy the model\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    predicted_labels, true_labels = zip(*[(model.forward(x.to(device)), y.to(device).item()) for x, y in test_dataloader])\n",
    "print(predicted_labels[20])\n",
    "print(true_labels[20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.5: Evaluate the performance of the model and visualize the confusion matrix. (5 points)\n",
    "\n",
    "You can use sklearns related function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.999\n"
     ]
    }
   ],
   "source": [
    "#### ADD YOUR CODE HERE ####\n",
    "correct = 0\n",
    "for pred, gold in zip(predicted_labels, true_labels):\n",
    "    if pred.argmax() == gold:\n",
    "        correct += 1\n",
    "        \n",
    "print(\"Accuracy: \" + str(correct / len(true_labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Finetune your classifier. (15 points)\n",
    "\n",
    "In the previous section, you have built a pretty good classifier for certain species of fish. Now we are going to use this trained classifier and adapt it to classify a new set of species:\n",
    "\n",
    "    'Hourse Mackerel\n",
    "    'Red Mullet',\n",
    "    'Red Sea Bream'\n",
    "    'Sea Bass'\n",
    "\n",
    "### Step 4.1: Set up the data for new species. (2 points)\n",
    "Overwrite the labels correspondances so they only incude the new classes and regenerate the datasets and dataloaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "Multiclass_labels_correspondances ={\n",
    "    'Hourse Mackerel': 0,\n",
    "    'Red Mullet': 1,\n",
    "    'Red Sea Bream': 2,\n",
    "    'Sea Bass': 3}\n",
    "\n",
    "LENDATA = 4000\n",
    "idxs_train,idxs_test = split_train_test(LENDATA, 0.8)\n",
    "\n",
    "# Dataloaders\n",
    "#### ADD YOUR CODE HERE ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4.2: Freeze the weights of all previous layers of the network except the last layer. (5 points)\n",
    "\n",
    "You can freeze them by setting the gradient requirements to ```False```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def freeze_till_last(model):\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "freeze_till_last(model)\n",
    "# Modify the last layer. This layer is not freezed.\n",
    "#### ADD YOUR CODE HERE ####\n",
    "\n",
    "# Loss function\n",
    "criterion = ...\n",
    "\n",
    "# Optimiser and learning rate\n",
    "lr = ...\n",
    "optimizer = ...\n",
    "\n",
    "# Number of iterations for training\n",
    "epochs = ...\n",
    "\n",
    "# Training batch size\n",
    "train_batch_size = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4.3: Train and test your finetuned model. (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'ellipsis' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_115201/1247923950.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Finetune the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;31m#### ADD YOUR CODE HERE ####\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'ellipsis' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "# Finetune the model\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    #### ADD YOUR CODE HERE ####\n",
    "    pass\n",
    "\n",
    "# Deploy the model on the test set\n",
    "#### ADD YOUR CODE HERE ####\n",
    "\n",
    "# Evaluate the performance\n",
    "#### ADD YOUR CODE HERE ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4.4: Did finetuning work? Why did we freeze the first few layers? (3 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ADD YOUR RESPONSE HERE ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
